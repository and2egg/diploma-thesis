

\section{Cloud Scenario}

%\subsection{Assumptions}
%
%\begin{itemize}
%
%\item Energy Elasticity
%
%One of the key assumptions for the cloud scenario is a reasonable level of server energy elasticity. It is defined as the difference between power values at idle and peak state regarding CPU utilization. 
%
%\end{itemize}


\section{Cloud Simulation}

\subsection{Benefits and limitations}

\subsection{Simulation assumptions}

\subsection{Expected results}




\section{Cost models}

%TODO reference section
As discussed before virtual machine migration is vital to draw benefits from the proposed approach. The targeted benefits of this approach are overall cost reductions in a geographically dispersed cloud environment. 

As energy prices are provided in e.g.~MegaWatt Hours (MWh) the amount of consumed energy per time is needed to calculate resulting costs. Typically several factors contribute to the overall energy consumption in a data center such as server power consumption or cooling infrastructure. One efficiency measure of data centers is the Power Usage Effectiveness (PUE) that indicates the power needed for auxiliary devices vs.~the power drawn solely from IT equipment (e.g.~servers or network hubs). 

Even though cooling expenses play a significant part in the overall energy costs this work only considers servers' power consumption as it is most significant to the proposed scenario. 

A server's power consumption consists of a static and a dynamic part \cite{liu2013performance}. The static part is defined as the power drawn when a server is idle i.e.~it does not run any computations. The dynamic part varies according to the current data processing on the server which can be derived from the current utilization of hardware components. 


\subsection{Cost of optimal assignments}


In \cite{de2013study} it is shown that the cost of optimal assignment is further reduced by increasing the number of locations with data centers assigned to different energy markets and/or by increasing the variability of energy prices. 






\section{Forecasting}

\subsection{Forecasting of electricity prices}

Since the occurrence of competitive energy markets forecasting of energy prices has been vital to utility operators. 

As it will be seen later the consideration of price forecasts in a multi electricity market environment can have beneficial side effects regarding energy costs. 


\subsection{General forecasting procedure}

\begin{itemize}

\item Selection of data sets (location, frequency)

\item Investigation of characteristics of the selected data sets (stationarity, variance)

\item Making series stationary, on demand

\item Evaluation of appropriate models to be applied to the given data sets

\item Model generation based on evaluation

\item Model evaluation and comparison (goodness of fit, accuracy, residual characteristics) 

\item Tests on in-sample and out-of-sample data

\item Testing different forecasting horizons

\item Comparison of results for different time sets and locations

\item Conclusion and model selection based on results

\end{itemize}


\subsection{Forecast types}

%Forecasting can be done in different ways, each having their own drawbacks and benefits. 

Forecasting methods can be classified into two categories: Qualitative and Quantitative forecasting. 

\subsubsection{Qualitative forecasting}
Qualitative forecasting methods assume that there is no or no relevant data that can be used to make forecasts. This is the case for forecasts that aim to predict i.e. the first year's sales of a new product line or the expected profit of a newly created startup company. In both examples there is no data available beforehand and the forecast has to be derived from already known conditions also called predictor variables. 

\subsubsection{Quantitative forecasting}
Quantitative forecasting methods are based on numerical historical data, from which forecasts are derived. It is assumed that the data's pattern over time stays the same or will behave in a similar way in the future. 

Quantitative forecasting methods can again be classified roughly into two categories: Cross-sectional forecasting and Time Series forecasting.

\begin{itemize}

\item Cross-sectional forecasts collect data from a single point in time to predict a value for an item not in the dataset. For example, prices of real estates in a specific area and point in time are collected and the price for another real estate is estimated based on predictor variables (i.e., size of properties and number of floors). 

\item Time series forecasts are solely based on historical data from the value to be predicted. No other variables are considered for the calculation, which on the one hand might miss some influential factors from external conditions but on the other hand appears to be more accurate regarding the output of the forecasts. 
In case of energy price forecasts external variables that change energy price behavior might include bids in auctions of wholesale energy markets, the current oil price or availability of renewable energy sources. However, in time series forecasts these variables are neglected to set the focus on the time series of historical energy prices. 

\end{itemize}


\subsection{Outline of forecasting models}

%TODO
Discussion of suitable models for price time series from various energy markets. 


\subsubsection{Random walk}

A random walk model is best suited for series that exhibit major fluctuations on a short term and where no apparent trend can be recognized \cite{makridakisforecasting}(pg. 461). Thus this model can be described by an accumulation of a random error over time (equation \ref{eq:random_walk}). 

\begin{equation}
Y_t = \sum e_t
\label{eq:random_walk}
\end{equation}

$Y_t$ denotes the value of the time series at time point $t$ whereas $e_t$ is a timeseries of random errors where the errors exhibit no correlation and are normally distributed \cite{makridakisforecasting}(pg. 461). 

In \cite{makridakisforecasting}(pg. 464) it is stated that the behavior of economic and business series in particular can be characterized by a random walk model since they show so-called cycles or random fluctuations around a possible trend. 

As it is impossible to accurately predict upcoming values or cycles of a random walk model by definition one of the most suitable prediction models is the ``naive forecast'' where the forecasted value of the upcoming timestamp is set equal to the last observed value (equation \ref{eq:naive_forecast}).

\begin{equation}
\hat{Y}_{t+i} = Y_t
\label{eq:naive_forecast}
\end{equation}



\subsubsection{ARIMA models}

\emph{ARIMA model generation}

See Box-Jenkins Methodology. 


\subsubsection{Neural Network models}

An approach of using artificial neural networks (ANN) as a model to forecast short term electricity prices is presented in \cite{szkuta1999electricity}. 


\paragraph{Forecast competition}

Neural network forecasting competition available from 2008\footnote{\url{http://www.neural-forecasting-competition.com/NN5/}}. 




\subsubsection{Forecasting benchmarks}



\subsection{Model selection techniques}


\subsubsection{Autocorrelation}

The autocorrelation function determines the existence or non-existence of correlation between lagged variables within a time series. 

That is, the correlation between a point in the time series to another point in the same time series is calculated where the gap between the two points is fixed by the given lag. 

The autocorrelation is calculated as an autocorrelation coefficient in equation \ref{eq:autocorr_coeff}

\begin{equation}
R_h = \frac{C_h}{C_0}
\label{eq:autocorr_coeff}
\end{equation}


where $C_h$ denotes the autocovariance function and $C_0$ the variance function 
(see equations \ref{eq:autocov_func} and \ref{eq:variance_func}). 


\begin{equation}
C_h = \frac{1}{N} \sum\limits_{t=1}^{N-h} (Y_t - \bar{Y}) (Y_{t+h} - \bar{Y})
\label{eq:autocov_func}
\end{equation}


\begin{equation}
C_0 = \frac{1}{N} \sum\limits_{t=1}^{N} (Y_t - \bar{Y})^2
\label{eq:variance_func}
\end{equation}


Explanation: The autocovariance function determines the (average) covariance between variables with the given lag while the variance function determines the maximum (average) variance over all timestamps (t=1 to N). 


See \url{http://www.itl.nist.gov/div898/handbook/eda/section3/autocopl.htm}


\subsubsection{White noise tests for residuals}

To test if residuals can be regarded as white noise, its auto correlation functions is examined. In case auto correlations are present it means that some values within the series depend on one another. In theory a true white noise series should not exhibit any auto correlations but since we operate on finite time series there will always be a few correlations present in the data\cite{makridakisforecasting}. 

It can be shown that the auto correlations of a white noise series have a sampling distribution similar to a gaussian distribution with zero mean and a standard deviation of $1/\sqrt{n}$ where $n$ denotes the total number of observations in the series\cite{makridakisforecasting}. 



\subsubsection{Box Cox transformation}

The Box Cox transformation is used to transform a series based on a non-normal distribution to a normal distribution. This is done via log and exponential transformations. 

See \url{http://www.isixsigma.com/tools-templates/normality/making-data-normal-using-box-cox-power-transformation/}





\subsection{Forecast accuracy measures}

There are different kinds of accuracy measures that can be applied to forecasting methods. \footnote{https://www.otexts.org/fpp/2/5}

\subsubsection{Scale-dependent error measures}

Scale dependent errors are absolute error measures given in the same scale as the dataset that is examined. Thus they are most useful when testing fc methods on the same dataset or on the same scale, respectively. 

\begin{itemize}
\item Mean absolute error (MAE)
The absolute error is defined by the value difference between a value of the dataset and its forecasted value. The mean absolute error returns the mean of all absolute errors on the dataset. [insert formula]

\item Root mean squared error (RMSE)
The root mean squared error takes the root of the sum of the squared forecast errors, and is scale dependent as well. [insert formula]

\end{itemize}

\subsubsection{Percentage error measures}

The most common percentage error is the mean absolute percentage error (MAPE) which is defined as the mean of percentage errors of a dataset. A percentage error is defined as $p_i=e_i/y_i * 100$. The mean absolute percentage error is then $mean(|pi|)$. 

The disadvantage of percentage error measures is that the value becomes infinite if there is some $y_i = 0$ on the dataset. Also, extreme values occur when the values on the dataset come close to zero. 


\subsubsection{Forecast error classification}

\begin{itemize}

\item Mean absolute error (MAE): \[ \frac{1}{n} \sum_{i=1}^{n} |\hat{y}_i - y_i| \] % \[\sum_{i=1}^{10} t_i\] %\sum(abs(predicted - actual)) / N

\item Mean squared error (MSE): \[ \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2 \] %$sum((predicted - actual)^2) / N$

\item Root mean squared error (RMSE): \[ \sqrt{\frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2}\] % $sqrt(sum((predicted - actual)^2) / N)$

\item Mean absolute percentage error (MAPE): \[ \frac{1}{n} \sum_{i=1}^{n} \left|\frac{\hat{y}_i - y_i}{y_i}\right| \] % $sum(abs((predicted - actual) / actual)) / N$

\item Direction accuracy (DAC): \[ \frac{count(sign(y_i - y_{i-1}) = sign(\hat{y}_i - \hat{y}_{i-1}))}{n} \]
% $count(sign(actual\_current - actual\_previous) == sign(pred\_current - pred\_previous)) / N$

\item Relative absolute error (RAE): \[  \frac{ \sum_{i=1}^{n} |\hat{y}_i - y_i|}{ \sum_{i=1}^{n} |\hat{y}_{i-1} - y_i|}   \]
 % $sum(abs(predicted - actual)) / sum(abs(previous\_target - actual))$

\item Root relative squared error (RRSE): \[  \frac{ \sqrt{ \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2 }} { \sqrt{ \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_{i-1} - y_i)^2 }} \]
 % $sqrt(sum((predicted - actual)^2) / N) / sqrt(sum(previous\_target - actual)^2 / N)$

\end{itemize}






%\subsection{Forecasting in cloud environments}


\section{SLA management}

As mentioned before the proposed approach relies on Virtual Machine migrations to shift load to data centers with currently cheaper energy prices. Each VM migration causes a temporary downtime depending on network bandwidth and VM memory \cite{liu2013performance} which has to be considered when applying this approach. 

Big cloud providers such as Google or Amazon commonly offer high availability services with a guaranteed availability of 99\% or even 99.9\%\footnote{Google Service Level Agreements \url{https://cloud.google.com/storage/sla}}\footnote{Amazon Service Level Agreements \url{https://aws.amazon.com/de/ec2/sla/}}. However when applying geotemporal VM migrations within a cloud environment it is not possible to keep availability at these levels due to VM downtimes. Therefore an alternative approach considering flexible SLA offers needs to be applied to accommodate possible downtimes \cite{luvcanin2014energy}. 


%\section{Virtual Machine migration}






%[TODO - move this paragraph to Simulation]

%In this thesis different energy markets have been chosen that exhibit a considerable degree of price volatility and are partly located in different timezones. Therefore different scenarios may show interesting characteristics due to time- and location based differences in energy prices. 

%A cost function is provided which calculates the total energy cost based on the energy price, the power used by a single server and the total number of running servers for each location. In addition constraints are defined s.t.~the workload assigned to a DC does not exceed its capacity and the total number of requests does not exceed a predefined threshold. 



%
%\section{Data center characteristics}
%
%\subsection{Key indicators in data centers}
%
%\subsection{Data management}
%
%\subsection{Best practices in federated cloud environments}




\section{Summary}



